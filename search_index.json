[["index.html", "Courses @ CRG: Reproducible research and data analysis with Linux containers and Nextflow pipelines Part 1 Welcome and about the course", " Courses @ CRG: Reproducible research and data analysis with Linux containers and Nextflow pipelines Sarah Bonnin1, Luca Cozzuto2, Toni Hermoso3, Julia Ponomarenko4 Part 1 Welcome and about the course This slow-paced hands-on course is designed for absolute beginners who want to start using containers and Nextflow pipelines to achieve reproducibility of data analysis. Linux containers allow the storage of code and applications in an host-independent lightweight environment. They became a fast and popular way to share and deploy applications in different environments. Nextflow is a powerful polyglot workflow language that, coupled with Docker and Singularity containers, provides a robust, scalable and reproducible way to run computational pipelines. CRG, sarah.bonnin@crg.eu↩︎ CRG, luca.cozzuto@crg.eu↩︎ CRG, toni.hermoso@crg.eu↩︎ CRG, julia.ponomarenko@crg.eu↩︎ "],["meet-the-organizers-and-instructors.html", "1.1 Meet the organizers and instructors", " 1.1 Meet the organizers and instructors Damjana Kastelic Anna Solé Toni Hermoso Luca Cozzuto Sarah Bonnin Julia Ponomarenko Emilio Palumbo Alessio Vignoli José Espinosa Leila Mansouri Athanasios Baltzis "],["about-the-course.html", "1.2 About the course", " 1.2 About the course 1.2.1 Outline The 4-day Containers and Nextflow course will train participants to use and build Docker and Singularity containers and Nextflow pipelines. It is designed to provide trainees with short and frequent hands-on sessions, while keeping theoretical sessions to a minimum. The course will be fully virtual via the Zoom platform. Trainees will work in a dedicated AWS environment. 1.2.2 Learning objectives About Linux containers: Locate and fetch Docker/Singularity images from dedicated repositories. Execute/Run a Docker/Singularity container from the command line. Build Docker container from an existing recipe. Design/Write a Docker recipe. Convert Docker to Singularity image. About Nextflow: Locate and fetch Nextflow pipelines from dedicated repositories. Execute/Run a Nextflow pipeline. Describe and explain Nextflow’s basic concepts. Test and modify a Nextflow pipeline. Implement short blocks of code into a Nextflow pipeline. Develop a Nextflow pipeline from scratch. Run pipeline in diverse computational environments (local, HPC, cloud ) 1.2.3 Prerequisite, Dates, time, location Being comfortable working with the CLI (command-line interface) in a Linux-based environment. Applicants are not expected to have used neither Linux containers nor Nextflow workflows before. Dates: May 2021: Containers: Monday 3rd, Tuesday 4th Nextflow: Monday 10th, Tuesday 11th Time: 9:30-17:30: Morning coffee break: 11:00-11:30am Lunch break: 1pm-2pm Afternoon coffee break: 3:30-4:00pm Location: virtual, via Zoom. "],["program.html", "1.3 Program", " 1.3 Program 1.3.1 May 3-4: Containers 1.3.1.1 Day 1: Docker Containers: introduction and history. Docker hub: find existing containers. Fetch (and build) an image from public registries (Docker Hub, Quay.io, etc.) Discovery of different relevant base images Rocker, continuumio, biocontainers.pro. Run Docker container based on an existing image (also run it interactively). Work with volumes and ports. Build an image from an existing recipe: Explain sections and keywords. Build options (e.g. cache or build variables). Write a Docker recipe and build an image from it. Upload image to registries. 1.3.1.2 Day 2: Singularity Singularity versus Docker. Differences, pros and cons for each system. Fetch (and build) Singularity images. Build from existing public registries. Build from local Docker instances. Run Singularity container (interactively). Understanding and working with volumes. Singularity build recipes. Advanced features: services, etc. 1.3.2 May 10-11: Nextflow 1.3.2.1 Day 3: Understand the code, run, modify Introduction to Nextflow Scripting Basic concepts: Channels, processes and workflows The work folder and its structure The log file Resuming pipelines 1.3.2.2 Day 4: Write and share Decoupling configuration and main script Run Nextflow with containers Run Nextflow in a computing cluster (e.g. HPC) Definition of computing requirements and queues Profiles Modules, subworkflows and reuse of the code Share Nextflow pipelines and good practices "],["introduction-to-containers.html", "Part 2 Introduction to containers ", " Part 2 Introduction to containers "],["what-are-containers.html", "2.1 What are containers ?", " 2.1 What are containers ? A Container can be seen as a minimal virtual environment that can be used in any Linux-compatible machine (and beyond). Using containers is time- and resource-saving as they allow: * Controlling for software installation and dependencies. * Reproducibility of the analysis. Containers allow us to use exactly the same versions of the tools. "],["virtual-machines-or-containers.html", "2.2 Virtual machines or containers ?", " 2.2 Virtual machines or containers ? Virtualisation Containerisation (aka lightweight virtualisation) Abstraction of physical hardware Abstraction of application layer Depends on hypervisor (software) Depends on host kernel (OS) Do not confuse with hardware emulator Application and dependencies bundled all together Enable virtual machines:Every virtual machine with an OS (Operating System) 2.2.1 Virtualisation Abstraction of physical hardware Depends on hypervisor (software) Do not confuse with hardware emulator Enable virtual machines: Every virtual machine with an OS (Operating System) 2.2.2 Containerisation (aka lightweight virtualisation) Abstraction of application layer Depends on host kernel (OS) Application and dependencies bundled all together 2.2.3 Virtual machines vs containers Source Pros and cons Virtualisation Containerisation PROS * Very similar to a full OS * With current solutions, high OS diversity * No need of full OS installation (less space) * Faster than virtual machines * Easier automation * Current solutions allow easier distribution of recipes. More portability. CONS * Need of more space and resources * Slower than containers * Not as good automating * Some cases might not be exactly the same as a full OS * With current solutions, still less OS diversity "],["history-of-containers.html", "2.3 History of containers", " 2.3 History of containers 2.3.1 chroot chroot jail (BSD jail): first concept in 1979 Notable use in SSH and FTP servers Honeypot, recovery of systems, etc. Source: https://sysopsio.wordpress.com/2016/09/09/jails-in-linux/ 2.3.2 Additions in Linux kernel cgroups (control groups), before “process containers” isolate resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes Linux namespaces one set of kernel resources restrict to one set of processes Source: https://sysopsio.wordpress.com/2016/09/09/jails-in-linux/ "],["docker.html", "Part 3 Docker ", " Part 3 Docker "],["introduction-to-docker.html", "3.1 Introduction to Docker", " 3.1 Introduction to Docker 3.1.1 What is Docker? Platform for developing, shipping and running applications Infrastructure as application / code First version: 2013 Company. Original dotCloud (2010), later named Docker Established Open Container Initiative As a software: Docker Community Edition Docker Enterprise Edition 3.1.2 Docker components Read-only templates Containers are run from them Images are not run Images have several layers 3.1.3 Images versus containers Image: set of layers. read-only templates. Inert. An instance of an image is called a container. When you start an image, you have a running container of this image. You can have many running containers of the same image. The image is the recipe, the container is the cake. https://stackoverflow.com/questions/23735149/what-is-the-difference-between-a-docker-image-and-a-container 3.1.4 Docker vocabulary Get all the docker commands: docker Get help on a particular command: docker run --help HANDS-ON What are the following commands doing? docker images docker export Answer here "],["using-existing-images.html", "3.2 Using existing images", " 3.2 Using existing images 3.2.1 Getting started Run the following in the terminal: docker images The docker images command lists the Docker images that you have on your computer. Now run the following: docker pull hello-world Run docker images again: now you see the “hello-world” image listed! docker pull imports an image from - by default - Docker hub. We will see in more details the docker images and docker run commands, but let’s first explore the Docker images repositories. 3.2.2 Explore Docker hub Images can be stored locally or shared in a registry. Docker hub is the main public registry for Docker images. Let’s search the keyword ubuntu: You can also search existing Docker images with the docker search command. Example: let’s look for images that have the keyword blast in their name or description. docker search blast Too many results? You can apply some filters: Minimum number of stars: docker search blast --filter stars=5 The image is an official build: docker search blast --filter is-official=true The image is an automated build: docker search blast --filter is-automated=true # Apply one filter docker search blast --filter stars=2 # Apply more than one filter docker search blast --filter is-automated=true --filter stars=2 HANDS-ON Use docker search to find a Docker image for the keyword ubuntu. Using the filters, answer the following questions: How many images are official builds? How many images have 3 or more stars? How many images are official builds AND have 3 or more stars? What is the NAME of the image with the highest number of stars? Answer # Official builds docker search ubuntu --filter is-official=true # 3 or more stars docker search ubuntu --filter stars=3 # Both filters docker search ubuntu --filter is-official=true --filter stars=3 3.2.3 docker pull: import an image Say we are now interested in the ubuntu image from Docker hub. We can retrieve it with docker pull. By default, we get the latest image / latest release. docker pull ubuntu You can choose the version of Ubuntu you are fetching: check the different tags on the website (latest is also a tag): Note: docker search doesn’t allow to search for tags. Let’s get the Ubuntu image with tag 18.04 (version 18.04 of Ubuntu = bionic): docker pull ubuntu:18.04 Where is the image now? As we have seen before, you can docker images in the terminal, to see a list of the most recently created images. docker images gives you information such as: Repository Tag Unique image ID Creation date Image size HANDS-ON Run docker images. How many images do you get? Pull the version 2.2.31 of the biocontainers/blast image What is the size of the blast image you just pulled? How many images do you get if you run docker images --all? What are those? Perhaps the documentation can help. Answer # Pull the blast image docker pull biocontainers/blast:2.2.31 # Run `docker images --all` docker images --all # intermediate images. 3.2.4 docker run: run image, i.e. start a container Now we want to use what is inside the image. docker run creates a fresh container (active instance of the image) from a Docker (static) image, and runs it. The format is: docker run image:tag command (command being a command called inside the image) We can start a container from the ubuntu tag 18.04 image, executing the command ls (stored in /bin in the container). docker run ubuntu:18.04 /bin/ls Now execute ls in your current working directory: is the result the same? You can execute any program/command that is stored inside the image: docker run ubuntu:18.04 /bin/whoami docker run ubuntu:18.04 cat /etc/issue You can either execute programs in the image from the command line (see above) or execute a container interactively, i.e. “enter” the container, with docker run -it. docker run -it ubuntu:18.04 /bin/bash If you want to leave and stop the container, type exit and ENTER. HANDS-ON Run the hello-world image: What is happening? Now run the blast image we previously pulled: Is something happening? Start again a container from the same blast image (not interactively), and run the blastp command. What happens? Start a container interactively from the same blast image: What is the default working directory? What is inside the directory? Where is the blastp program located in the image? Exit the container. Answer # Run the hello-world image docker run hello-world # Run the blast image docker run biocontainers/blast:2.2.31 # Start again a container from the same blast image, and run the path to the blastp command: docker run biocontainers/blast:2.2.31 blastp # Start a container interactively from the same blast image: docker run -ti biocontainers/blast:2.2.31 # What is the default working directory? pwd; ls # Where is the `blastp` program located in the image? which blastp # Exit the container. exit You can run the container as daemon (in background), instead of the default foreground running, with the --detach parameter: docker run --detach ubuntu:18.04 tail -f /dev/null Run container as daemon (in background) with a given name: docker run --detach --name myubuntu ubuntu:18.04 tail -f /dev/null 3.2.5 docker ps: check containers status List running containers: docker ps List all containers (whether they are running or not): docker ps -a Each container has a unique ID. 3.2.6 docker exec: execute process in running container Difference between docker run and docker exec: docker run creates a temporary container, runs the command and stops the container. docker exec needs an already running container to query the command (i.e. a detached container). docker exec myubuntu uname -a Interactively docker exec -it myubuntu /bin/bash 3.2.7 docker stop, start, restart: actions on container Stop a running container with docker stop. # check the list of running containers docker ps # stop the myubuntu container docker stop myubuntu # check the list of all containers docker ps -a Start a stopped container (does NOT create a new one): docker start myubuntu docker ps -a Restart a running container: docker restart myubuntu docker ps -a Run with restart enabled (by default, Docker does not automatically restart the container when it exits). In the example below we start a detached container named “myubuntu2” with the unless-stopped restart policy: restart the container unless it is explicitly stopped or Docker itself is stopped or restarted. docker run --restart=unless-stopped --detach --name myubuntu2 ubuntu:18.04 tail -f /dev/null Restart policies: no (default), always, on-failure, unless-stopped Update restart policy: docker update --restart unless-stopped myubuntu HANDS-ON Start a container from the “hello-world” image in the background. Give it a name. Is your container running? Can you explain why (or why not)? Start another detached container from the same image (with a new name), with the always restart policy. Is the container running? Answer # start a &quot;detached&quot; container (in the background) docker run --detach --name helloworld1 hello-world # start a &quot;detached&quot; container with the &quot;--restart=always&quot; option docker run --detach --restart=always --name helloworld2 hello-world 3.2.8 docker rm, docker rmi: clean up! docker rm is used to remove a container (set -f is the container is running, to force the removal): docker rm myubuntu docker rm -f myubuntu docker rmi is used to remove an image: docker rmi ubuntu:18.04 HANDS-ON Remove any container (whether it is running or not). Remove the “hello-world” image. Answer # check all containers docker ps -a # remove by their ID: docker rm -f CONTAINER1_ID CONTAINER2_ID ... # remove the &quot;hello-world&quot; image docker rmi hello-world 3.2.8.1 Major clean Check used space: docker system df Remove unused containers (and others) - DO WITH CARE docker system prune Remove ALL non-running containers, images, etc. - DO WITH MUCH MORE CARE!!! docker system prune -a Reference "],["exercise-1-docker-as-a-user.html", "3.3 Exercise 1 - Docker as a user", " 3.3 Exercise 1 - Docker as a user In breakout rooms, do the following exercise: alpine image. Search and pull the alpine image (tag 3.12) - it is an official build. Can you run a container from this image and make it print a “hello world” message? Now run a container interactively from the same image. Run whoami in the container. Exit the container and run whoami on the host machine: do you get the same output? Restart the container you just exited: Is it now running? Make the container execute the command ls. Stop the container. Remove the alpine image and all its containers (running or stopped) Answer # Search and pull the alpine image (tag 3.12) - it is an official build. docker search alpine --filter is-official=true docker pull alpine:3.12 # Can you run a container from this image and make it print a “hello world” message? docker run alpine:3.12 echo &quot;hello world&quot; # Now run a container **interactively** from the same image. docker run -ti alpine:3.12 # Run `whoami` whoami # Exit the container. exit # Restart the container you just exited: is it now running? docker restart CONTAINER_ID # find it with `docker ps -a` # Make the container execute the command `ls` docker exec CONTAINER_ID ls # Stop the container docker stop CONTAINER_ID # Remove the alpine image and all its containers (running or stopped) docker rmi alpine:3.12 docker rm CONTAINER_ID # check all containers with `docker ps -a` imagemagick Pull the ìmagemagick image that is official and that has the highest number of stars Check the version of the convert command. Start a container interactively. Inside the container: download this png image Convert it to .jpg using the convert command of imagemagick (format; convert image.png image.jpg) Exit the container Copy the jpg image back from the stopped container! Explore docker cp. Answer # Pull image docker pull acleancoder/imagemagick-full # Check version of `convert` docker run acleancoder/imagemagick-full convert --version # Start interactive container docker run -it acleancoder/imagemagick-full # fetch png image &gt; wget https://pbs.twimg.com/profile_images/1273307847103635465/lfVWBmiW_400x400.png # convert to jpg &gt; convert lfVWBmiW_400x400.png myimage.jpg # exit container # fetch container ID with `ps -a` and use `docker cp` to copy jpg file from the stopped container to the host docker cp *CONTAINER_ID*:/myimage.jpg . "],["linux-packages.html", "3.4 Linux packages", " 3.4 Linux packages In the next topic, we will create Docker images. For this to go smoothly, you should know your base system, and how to interact with it (update, upgrade and install packages for example). In Linux-based environment, software is usually distributed in the form of packages, kept in repositories. With different operating systems come different tools to fetch and install packages: Operating System Format Tool(s) Debian / Ubuntu / Linux Mint / Raspbian .deb apt, apt-cache, apt-get, dpkg CentOS (RedHat) .rpm yum Fedora (RedHat) .rpm dnf FreeBSD .txz make, pkg Alpine apk More on the subject in this post Note that the environment you are working on for this course is CentOS-based During this course, we will build images based on Ubuntu and CentOS. 3.4.1 Update and upgrade packages In Ubuntu: apt-get update &amp;&amp; apt-get upgrade -y In CentOS: yum check-update &amp;&amp; yum update -y 3.4.2 Search and install packages: In Ubuntu: apt search libxml2 apt install -y libxml2-dev In CentOS: yum search libxml2 yum install -y libxml2-devel.x86_64 Note the -y option that we set for updating and for installing. It is an important option in the context of Docker: it means that you answer yes for all questions, regarding whether you want to confirm the installation, for example. Note that Conda is a nice and easy way to install software: Anaconda Conda-forge Bioconda "],["docker-recipes.html", "3.5 Docker recipes", " 3.5 Docker recipes A Docker recipe contains a set of instructions and commands that will be used to create/build a Docker image. 3.5.1 Writing recipes and building images All commands should be saved in a text file, named by default Dockerfile. Instruction What it does FROM Sets the base image. RUN Commands to run. MAINTAINER Docker image maintainer. WORKDIR Sets the working directory for the container’s building instructions. SHELL Changes the default shell ADD / COPY Copy files from source to destination. ARG Sets variables that can be used as the image is built. ENV Sets environment variables. Persists when container is run. ENTRYPOINT Helps configure the container as an executable. CMD Can provide a default executable or arguments to the ENTRYPOINT executable. VOLUME Creates a mount point for an external volume. EXPOSE Exposes network ports on the container. Reference 3.5.1.1 Basic instructions Each row in the recipe corresponds to a layer of the final image. FROM: parent image. Typically, an operating system. This is the base layer. FROM ubuntu:18.04 RUN: the command to execute inside the image filesystem. Think about it this way: every RUN line is essentially what you would run to install programs on a freshly installed Ubuntu OS. RUN apt install wget A basic recipe that is taking the ubuntu:18.04 image as a base layer, that updates and upgrades Linux packages, and that installs wget: FROM ubuntu:18.04 RUN apt update &amp;&amp; apt -y upgrade RUN apt install -y wget HANDS-ON Explore this Dockerfile: What is the base layer? What is being installed in the image? How? Answer Base layer: biocontainers/biocontainers:v1.0.0_cv4 Tool blast is installed using conda. 3.5.1.2 Building images from recipes docker build will create/build a Docker image from a Docker recipe. Save the following commands: FROM ubuntu:18.04 RUN apt update &amp;&amp; apt -y upgrade RUN apt install -y wget in a file named Dockerfile docker build implicitely looks for a file named Dockerfile in the current directory: docker build . Same as: docker build --file Dockerfile . Syntax: --file / -f . stands for the context (in this case, current directory) of the build process. This makes sense if copying files from filesystem, for instance. IMPORTANT: Avoid contexts (directories) overpopulated with files (even if not actually used in the recipe). You can define a specific name for the image during the build process. Syntax: -t imagename:tag. If not defined :tag default is latest. docker build -t mytestimage . # same as: docker build -t mytestimage:latest . The last line of installation should be Successfully built …: then you are good to go. Check with docker images that you see the newly built image in the list… Then let’s check the ID of the image and run it! # Get the ID with docker images docker images # Run/start a container using the ID or name docker run f9f41698e2f8 docker run mytestimage 3.5.1.3 More instructions MAINTAINER Who is maintaining the container? MAINTAINER Toni Hermoso Pulido &lt;toni.hermoso@crg.eu&gt; WORKDIR: all subsequent actions will be executed in that working directory WORKDIR ~ SHELL: allows the default shell used for the shell form of commands to be overridden. Use bash as the default shell: SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;] ADD, COPY: add files to the image filesystem Difference between ADD and COPY explained here and here COPY: lets you copy a local file or directory from your host (the machine from which you are building the image) ADD: same, but ADD works also for URLs, and for .tar archives that will be automatically extracted upon being copied. # COPY source destination COPY ~/.bashrc . ENV, ARG: run and build environment variables Difference between ARG and ENV explained here. ARG values: available only while the image is built. ENV values: available for the future running containers. You can use ARG, for example, to specify the version of the base layer you want to use: With a default value ARG UbuntuVersion=18.04 FROM ubuntu:${UbuntuVersion} Without a default value (i.e. the user is expected to provide it upon building) ARG UbuntuVersion FROM ubuntu:${UbuntuVersion} Provide a value for UbuntuVersion as you build the image with --build-arg: docker build --build-arg UbuntuVersion=20.04 . You can also use ARG to build a specific software version in the image: FROM ubuntu:18.04 ARG PyVersion=2.7 RUN apt update &amp;&amp; apt upgrade -y RUN apt install -y python${PyVersion} We can save this recipe in a file called Dockerfile_ARG. Build the image to get the default version of Python (version 2.7, as given by ARG in the recipe): docker build -t py27 -f Dockerfile_ARG . Install version 3.8 of Python instead (give the argument via the --build-arg option): docker build --build-arg PyVersion=3.8 -t py38 -f Dockerfile_ARG . Now run the image and check if the python of the correct version is installed: docker run py27 python2.7 --help docker run py38 python3.8 --help CMD, ENTRYPOINT: command to execute when generated container starts The ENTRYPOINT specifies a command that will always be executed when the container starts. The CMD specifies arguments that will be fed to the ENTRYPOINT. In the example below, when the container is run without an argument, it will execute echo \"hello world\" (default). If it is run with the argument nice it will execute echo \"nice\". The argument given in CMD will be overridden. FROM ubuntu:18.04 ENTRYPOINT [&quot;/bin/echo&quot;] CMD [&quot;hello world&quot;] A more complex recipe (save it in a text file named Dockerfile_ubuntu!): FROM ubuntu:18.04 MAINTAINER Toni Hermoso Pulido &lt;toni.hermoso@crg.eu&gt; SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;] WORKDIR ~ RUN apt-get update &amp;&amp; apt-get -y upgrade RUN apt-get install -y wget ENTRYPOINT [&quot;/usr/bin/wget&quot;] CMD [&quot;https://cdn.wp.nginx.com/wp-content/uploads/2016/07/docker-swarm-hero2.png&quot;] Build the image: docker build -f Dockerfile_ubuntu . Try to run it without and with an argument: # Remember to check the image ID with `docker images` docker run f9f41698e2f8 # with an argument docker run f9f41698e2f8 https://cdn-images-1.medium.com/max/1600/1*_NQN6_YnxS29m8vFzWYlEg.png 3.5.2 docker tag Use docker tag in order to tag a local image that has - for example - ID “f9f41698e2f8” into the “ubuntu_wget” image name repository with version/tag “1.0”: docker tag f9f41698e2f8 --tag ubuntu_wget:1.0 If the version/tag is not specified, it defaults to latest. 3.5.3 Build cache Every line of a Dockerfile is actually an image/layer by itself (you can see all images with docker images --all. Modify for instance the last bit of the previous image recipe (Dockerfile_ubuntu) (let’s change the image URL) and rebuild it (even with a different name/tag): FROM ubuntu:18.04 MAINTAINER Toni Hermoso Pulido &lt;toni.hermoso@crg.eu&gt; WORKDIR ~ RUN apt-get update &amp;&amp; apt-get -y upgrade RUN apt-get install -y wget ENTRYPOINT [&quot;/usr/bin/wget&quot;] CMD [&quot;https://cdn-images-1.medium.com/max/1600/1*_NQN6_YnxS29m8vFzWYlEg.png&quot;] docker build -t mytestimage2 -f Dockerfile_ubuntu . It will start from the last line, and not re-run commands before the modification that were successfully built. This is OK most of the times and very convenient for testing and trying new steps, but it may lead to errors when versions are updated (either FROM image or included packages). It is beneficial to start from scratch with --no-cache option when building the image. docker build --no-cache -t mytestimage2 -f Dockerfile_ubuntu . "],["exercise-2-docker-recipes-and-build-images.html", "3.6 Exercise 2 - Docker recipes and build images", " 3.6 Exercise 2 - Docker recipes and build images In breakout rooms, do the following exercise: figlet recipe. Write a docker recipe in a file Dockerfile_figlet that: is based on ubuntu:18.04 echoes “I love containers” by default or any other word/sentence, if given as the argument. Build the image (give the image the name of your choice) Run the image: with no argument. with “Docker course” as an argument. Modify the Dockerfile_figlet: add the MAINTAINER field update and upgrade Ubuntu packages. install the figlet program change “echo” to “figlet” in the ENTRYPOINT Build the image. Run the new image, with the default parameters, then with “Docker course” as an argument. Answer Recipe saved in Dockerfile_figlet: FROM ubuntu:18.04 ENTRYPOINT [&quot;echo&quot;] CMD [&quot;I love containers&quot;] Build: docker build --file Dockerfile_figlet -t mytest . Run the image: # with no argument docker run mytest # with argument &quot;Docker course&quot; docker run mytest &quot;Docker course&quot; Recipe: FROM ubuntu:18.04 MAINTAINER Name Surname &lt;name.surname@mail.com&gt; RUN apt-get update &amp;&amp; apt-get upgrade -y RUN apt-get install -y figlet ENTRYPOINT [&quot;figlet&quot;] CMD [&quot;I love containers&quot;] Build the image. docker build --no-cache --file Dockerfile_figlet -t mytestfiglet . Run the new image, with the default parameter, then with “Docker course” as an argument. # run with no argument docker run mytestfiglet # run with argument &quot;Docker course&quot; docker run mytestfiglet &quot;Docker course&quot; Random numbers Copy the following short bash script in a file called random_numbers.bash. #!/usr/bin/bash seq 1 1000 | shuf | head -$1 This script outputs **random intergers from 1 to 1000**: the number of integers selected is given as the **first argument**. * Write a recipe for an image: * Based on **centos:7** * That will execute this script (with `bash`) when it is run, giving it **2** as a default argument (i.e. **outputs 2 random integers**): the default can be changed as the image is run. * Build the image. * Start a container with the default argument, then try it with another argument. Answer Recipe (in Dockerfile_RN): FROM centos:7 MAINTAINER Name Surname &lt;name.surname@mail.com&gt; # Copy script from host to image COPY random_numbers.bash . # Make script executable RUN chmod +x random_numbers.bash # As the container starts, &quot;random_numbers.bash&quot; is run ENTRYPOINT [&quot;/usr/bin/bash&quot;, &quot;random_numbers.bash&quot;] # default argument (that can be changed on the command line) CMD [&quot;2&quot;] Build and run: docker build -f Dockerfile_RN -t random_numbers . docker run random_numbers docker run random_numbers 10 3.6.1 Additional commands docker inspect: Get details from containers (both running and stopped). Things such as IPs, volumes, etc. docker logs: Get console messages from running containers. Useful when using with web services. docker commit: Turn a container into an image. It make senses to use when modifying container interactively. However this is bad for reproducibility if no steps are saved. Good for long-term reproducibility and for critical production environments: docker save: Save an image into a tar archive. docker export: Save a container into a tar archive. docker import: Import a tar archive into an image. HANDS-ON Save a previously created image into a tar archive (Look at the command’s options for help) Answer docker save -o random_numbers.tar random_numbers "],["volumes.html", "3.7 Volumes", " 3.7 Volumes Docker containers are fully isolated. It is necessary to mount volumes in order to handle input/output files. Syntax: --volume/-v host:container We can pull the following image to illustrate: docker pull biocontainers/fastqc:v0.11.9_cv7 FastQC is a tool that runs a quality control on .fastq files (a file format that stores nucleotide sequences and their corresponding quality scores). # Create directory and empty file mkdir datatest touch datatest/test # Run a container in the background (--detach) and mount the local volume &quot;datatest&quot; (we map it to directory /scratch inside the container) docker run --detach --volume $(pwd)/datatest:/scratch --name fastqc_container biocontainers/fastqc:v0.11.9_cv7 tail -f /dev/null # Execute the container interactively docker exec -ti fastqc_container /bin/bash &gt; ls -l /scratch &gt; exit HANDS-ON Copy the 2 fastq files from the Github repository and place them in mounted directory. Run fastqc interactively (inside container): fastqc /scratch/*.gz Run fastqc outside the container Answer # Download test fastq files (manually or using the following commands) and place them in &quot;datatest&quot;: wget -O - https://github.com/biocorecrg/CoursesCRG_Containers_Nextflow_May_2021/blob/main/testdata/B7_H3K4me1_s_chr19.fastq.gz?raw=true &gt; datatest/B7_H3K4me1_s_chr19.fastq.gz wget -O - https://github.com/biocorecrg/CoursesCRG_Containers_Nextflow_May_2021/blob/main/testdata/B7_input_s_chr19.fastq.gz?raw=true &gt; datatest/B7_input_s_chr19.fastq.gz # Mount volumes and start a detached container docker run --detach -ti --volume $(pwd)/datatest:/scratch --name fastqc_container_test biocontainers/fastqc:v0.11.9_cv7 # Execute container interactively and run fastqc docker exec -ti fastqc_container_test /bin/bash &gt; fastqc /scratch/*.gz # Run fastqc outside the container # One by one docker exec fastqc_container_test fastqc /scratch/B7_H3K4me1_s_chr19.fastq.gz docker exec fastqc_container_test fastqc /scratch/B7_input_s_chr19.fastq.gz # All (using wildcard *) docker exec fastqc_container_test bash -c &#39;ls /scratch/*gz&#39; "],["ports.html", "3.8 Ports", " 3.8 Ports The same as with volumes, but with ports, to access Internet services. Syntax: --publish/-p host:container docker run --detach --name webserver nginx curl localhost:80 docker exec webserver curl localhost:80 docker rm -f webserver docker run --detach --name webserver --publish 80:80 nginx curl localhost:80 docker rm -f webserver docker run --detach --name webserver -p 8080:80 nginx curl localhost:80 curl localhost:8080 docker exec webserver curl localhost:80 docker exec webserver curl localhost:8080 docker rm -f webserver "],["integrative-example.html", "3.9 Integrative example", " 3.9 Integrative example 3.9.1 FASTQC Web Application We work in a dummy FASTQC Web service We place B7_input_s_chr19.fastq.gz file from available datasets in $HOME/myscratch mdkir -p $HOME/myscratch cp testdata/* $HOME/myscratch cd containers/docker/fastqc_www docker build -t fastqcwww -f Dockerfile ../../scripts/fastqc docker run -d -v $HOME/myscratch:/scratch -p 8083:3838 --name myfastqc fastqcwww Example query from the browser: http://mymachine-address-here:3838/?file=B7_input_s_chr19.fastq NOTE about context here … to see if this has been explained before Replace your mymachine-address-here for machine provided address or 127.0.0.1/localhost in case you were trying it from your own machine. 3.9.2 Shiny Application cd containers/docker/shiny docker build -t shinyapp -f Dockerfile ../../scripts docker run -d -v $(pwd)/../../scripts/shiny:/srv/shiny-server/myserver -p 3838:3838 --name myserver shinyapp Check the result from the browser http://mymachine-address-here:3838 In the CMD execution you see the host 0.0.0.0, this a normal approach to indicate association to any IP address from the machine you launch the program. In a very machine you can have many routings and networks associated (e.g., more than one Ethernet plug) "],["publish-images.html", "3.10 Publish images", " 3.10 Publish images 3.10.1 Push an image Figure 3.1: PUSH. Steve Snodgrass. CC-BY, Source: https://www.flickr.com/photos/stevensnodgrass/6117660537/ Generation of an access key in Docker Hub docker login (base) [ec2-user@ip-172-31-47-200 ~]$ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don&#39;t have a Docker ID, head over to https://hub.docker.com to create one. Username: toniher Password: WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store More details … 3.10.2 Automatic builds Creation of Github repository Addition of a Dockerfile Configuration of an automated build from Docker Hub "],["breakout-exercises-with-docker-3.html", "3.11 Breakout exercises with Docker - 3", " 3.11 Breakout exercises with Docker - 3 Simple BLAST web application. Including mount/volume and port. Description below If time push resulting image to Docker Hub "],["singularity.html", "Part 4 Singularity", " Part 4 Singularity "],["introduction-to-singularity.html", "4.1 Introduction to Singularity", " 4.1 Introduction to Singularity Focus: Reproducibility to scientific computing and the high-performance computing (HPC) world. Origin: Lawrence Berkeley National Laboratory. Later spin-off: Sylabs Version 1.0 -&gt; 2016 More information: https://en.wikipedia.org/wiki/Singularity_(software) 4.1.1 Singularity architecture Strengths Weaknesses No dependency of a daemon At the time of writing only good support in LinuxMac experimental. Desktop edition. Only running Can be run as a simple userAvoids permission headaches and hacks For some features you need root account (or sudo) Image/container is a file (or directory) More easily portable Two types of images:Read-only (production)Writable (development, via sandbox) 4.1.2 Strengths No dependency of a daemon Can be run as a simple user Avoid permission headaches and hacks Image/container is a file (or directory) More easily portable Two type of images Read-only (production) Writable (development, via sandbox) 4.1.3 Weaknesses At the time of writing only good support in Linux Mac experimental. Desktop edition. Only running For some features you need root account (or sudo) "],["build-process.html", "4.2 Build process", " 4.2 Build process 4.2.1 Examples 4.2.1.1 Through registries Docker Hub https://hub.docker.com/r/biocontainers/fastqc singularity build fastqc-0.11.9_cv7.sif docker://biocontainers/fastqc:v0.11.9_cv7 Biocontainers Via quay.io https://quay.io/repository/biocontainers/fastqc singularity build fastqc-0.11.9.sif docker://quay.io/biocontainers/fastqc:0.11.9--0 Via Galaxy project prebuilt images singularity pull --name fastqc-0.11.9.sif https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0 Docker Daemon (local Docker image) singularity build fastqc-web-0.11.9.sif docker-daemon://fastqcwww Also Singularity registries (not so popular so far) https://cloud.sylabs.io/library 4.2.2 Sandboxing singularity build --sandbox ./sandbox docker://ubuntu:18.04 touch sandbox/etc/myetc.conf singularity build sandbox.sif ./sandbox 4.2.3 Singularity recipes Saved in a .sif file? 4.2.3.1 Docker bootstrap What does itmean (bootstrap)? BootStrap: docker From: biocontainers/fastqc:v0.11.9_cv7 %runscript echo &quot;Welcome to FastQC Image&quot; fastqc --version %post echo &quot;Image built&quot; sudo singularity build fastqc.sif docker.singularity 4.2.3.2 Debian bootstrap BootStrap: debootstrap OSVersion: bionic MirrorURL: http://fr.archive.ubuntu.com/ubuntu/ Include: build-essential curl python python-dev openjdk-11-jdk bzip2 zip unzip %runscript echo &quot;Welcome to my Singularity Image&quot; fastqc --version multiqc --version bowtie --version %post FASTQC_VERSION=0.11.9 MULTIQC_VERSION=1.9 BOWTIE_VERSION=1.3.0 cd /usr/local; curl -k -L https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v${FASTQC_VERSION}.zip &gt; fastqc.zip cd /usr/local; unzip fastqc.zip; rm fastqc.zip; chmod 775 FastQC/fastqc; ln -s /usr/local/FastQC/fastqc /usr/local/bin/fastqc cd /usr/local; curl --fail --silent --show-error --location --remote-name https://github.com/BenLangmead/bowtie/releases/download/v$BOWTIE_VERSION/bowtie-${BOWTIE_VERSION}-linux-x86_64.zip cd /usr/local; unzip -d /usr/local bowtie-${BOWTIE_VERSION}-linux-x86_64.zip cd /usr/local; rm bowtie-${BOWTIE_VERSION}-linux-x86_64.zip cd /usr/local/bin; ln -s ../bowtie-${BOWTIE_VERSION}-linux-x86_64/bowtie* . curl --fail --silent --show-error --location --remote-name https://bootstrap.pypa.io/get-pip.py python get-pip.py pip install numpy matplotlib pip install -I multiqc==${MULTIQC_VERSION} echo &quot;Biocore image built&quot; %labels Maintainer Biocorecrg Version 0.1.0 sudo singularity build fastqc-multi-bowtie.sif debootstrap.singularity It’s possible to sign cryptographically your images, so third parties can verify they are coming from their actual authors. This implies some matters beyond the scope of this course, but you have some details if interested at: https://sylabs.io/guides/3.7/user-guide/signNverify.html "],["remote-building.html", "4.3 Remote building", " 4.3 Remote building This allows us to build a Singularity image without using our own computer. This is convenient if your machine has not many resources. We need to create a https://cloud.sylabs.io/ account first. singularity remote login In order to use it our username and passwords we generate a token at https://cloud.sylabs.io/auth/tokens and copy its contents to a file (e.g., singularity.token). Then we use in the command-line below: singularity remote login --tokenfile singularity.token Output (base) [ec2-user@ip-172-31-47-200 ~]$ singularity remote login –tokenfile singularity.token INFO: Access Token Verified! INFO: Token stored in /home/ec2-user/.singularity/remote.yaml For safety reasons, remove the token from Cloud Sylabs interface after the course. Follow the same philosophy for similar circumstances. Keep an only token for a specific environment or action. Once we are authenticated with our access token, we can build an image: singularity build --remote focal.sif docker://ubuntu:focal Since we are already authenticated, we can take advantage to push a Singularity image in our account, similarly to what we did with Docker. We use -U so we do not sign the image. singularity push -U focal.sif library://toniher/test/focal.sif:latest IMPORTANT: Unless it is not defined in advance as private from Cloud Sylabs interface, pushed image will be public and there does not seem an easy way to turn it private. As an interesting feature, you can actually sign your images after you pushed them from Cloud Sylabs web interface by using a key associated to your account (which can also be created there). More details at https://sylabs.io/guides/3.7/user-guide/cloud_library.html "],["run-and-execution-process.html", "4.4 Run and execution process", " 4.4 Run and execution process "],["singularity-shell.html", "4.5 Singularity shell", " 4.5 Singularity shell Interactive shell. singularity shell fastqc-multi-bowtie.sif 4.5.1 Singularity exec Execute commands. Common approach in HPC uses. singularity exec fastqc-multi-bowtie.sif fastqc 4.5.2 Singularity run Execute runscript from recipe definition. Not so common for HPC uses. More for instances (servers). singularity run fastqc-multi-bowtie.sif 4.5.3 Environment control singularity shell -e fastqc-multi-bowtie.sif singularity exec -e fastqc-multi-bowtie.sif fastqc singularity run -e fastqc-multi-bowtie.sif Compare env command with and without -e modifier. singularity exec fastqc-multi-bowtie.sif env singularity exec -e fastqc-multi-bowtie.sif env 4.5.4 Execute from sandboxed images / directories singularity exec ./sandbox ls -l /etc/myetc.conf # We can see file created in the directory before singularity exec ./sandbox bash -c &#39;apt-get update &amp;&amp; apt-get install python&#39; # We cannot install python singularity exec --writable ./sandbox bash -c &#39;apt-get update &amp;&amp; apt-get install python&#39; # We needed to add writable parameter 4.5.5 Execute straight from a registry singularity exec docker://ncbi/blast:2.10.1 blastp -version "],["bind-paths-aka-volumes.html", "4.6 Bind paths (aka volumes)", " 4.6 Bind paths (aka volumes) Paths of host system mounted in the container Default ones, no need to mount them explicitly (for 3.7.x): $HOME , /sys:/sys , /proc:/proc, /tmp:/tmp, /var/tmp:/var/tmp, /etc/resolv.conf:/etc/resolv.conf, /etc/passwd:/etc/passwd, and $PWD https://sylabs.io/guides/3.7/user-guide/bind_paths_and_mounts.html For others, need to be done explicitly (syntax: host:container) mkdir testdir touch testdir/testout singularity shell -e -B ./testdir:/scratch fastqc-multi-bowtie.sif &gt; touch /scratch/testin &gt; exit ls -l testdir "],["exercises.html", "4.7 Exercises:", " 4.7 Exercises: Using the 2 fastqc available files, process them outside and inside the mounted directory. "],["instances.html", "4.8 Instances", " 4.8 Instances Also know as services. Despite Docker it is still more convenient for these tasks, it allows enabling thing such as webservices (e.g., via APIs) in HPC workflows. Simple example: Bootstrap: docker From: library/mariadb:10.3 %startscript mysqld sudo singularity build mariadb.sif mariadb.singularity mkdir -p testdir mkdir -p testdir/db mkdir -p testdir/socket singularity exec -B ./testdir/db:/var/lib/mysql mariadb.sif mysql_install_db singularity instance start -B ./testdir/db:/var/lib/mysql -B ./testdir/socket:/run/mysqld mariadb.sif mydb singularity instance list singularity exec instance://mydb mysql -uroot singularity instance stop mydb When exposing ports below 1024 in a LINUX machine, administrator (sudo) privileges are needed. Historical reference: https://www.w3.org/Daemon/User/Installation/PrivilegedPorts.html Specially with instances (but not only), if you encounter some permission problems you may need to enable write permissions in the image. If you do not want changes to persist in the image, you may want to use the –writable-tmpfs option. Changes are stored in an in-memory temporary filesystem which is discarded as soon as the service stops. More details about special storage options with Singularity: https://sylabs.io/guides/3.7/user-guide/persistent_overlays.html More information: https://sylabs.io/guides/3.7/user-guide/running_services.html https://sylabs.io/guides/3.7/user-guide/networking.html "],["troubleshooting.html", "4.9 Troubleshooting", " 4.9 Troubleshooting singularity --help 4.9.1 Fakeroot Singularity permissions are an evolving field. If you don’t have access to sudo, it might be worth considering using --fakeroot/-f parameter. More details at https://sylabs.io/guides/3.7/user-guide/fakeroot.html 4.9.2 Singularity cache directory $HOME/.singularity It stores cached images from registries, instances, etc. If problems may be a good place to clean. When running sudo, $HOME is /root. 4.9.3 Global singularity configuration Normally at /etc/singularity/singularity.conf or similar (e.g preceded by /usr/local/ is Singularity is installed manually) It can only be modified by users with administration permissions Worth noting bind path lines, which point default mounted directories in containers as commented in bind paths section 4.9.4 Breakout exercises with Singularity Example running BLAST commands in different ways Example running nginx as a service (NGINX its actually the custom example in Singularity page) Retrieve NGINX version from the instance Share a simply HTML page NGINX is serving web pages by default at the following location: /usr/share/nginx/html "],["nextflow-day-1.html", "Part 5 Nextflow day 1", " Part 5 Nextflow day 1 A DSL for data-driven computational pipelines. www.nextflow.io "],["what-is-nextflow.html", "5.1 What is Nextflow?", " 5.1 What is Nextflow? Nextflow is a domain specific language for workflow orchestration that stems from Groovy. It enables scalable and reproducible workflows using software containers. It was developed at CRG in the Lab of Cedric Notredame by Paolo Di Tommaso https://github.com/pditommaso. The Nextflow documentantion is available here and you can ask help to the community using their gitter channel Nextflow has been currently upgrade from DLS1 version to DLS2. In this course we will use exclusively DLS2. "],["what-is-nextflow-for.html", "5.2 What is Nextflow for?", " 5.2 What is Nextflow for? It is for making pipelines without caring about parallelization, dependencies, intermediate file names, data structures, handling exceptions, resuming executions etc. It was published in Nature Biotechnology in 2017. There is a growing number of publication mentioning Nextflow in PubMed, since many bioinformaticians are starting to write their pipeline in Nextflow. Here a curated list of Nextflow pipelines And here a group of pipelines written in a collaborative way from project NF-core Some pipelines written in Nextflow are being used for SARS-Cov-2 analysis like: the one from the artic Network: ncov2019-artic-nf the one used from the CRG / EGA viral Beacon, Master of Pores the nf-core pipeline viralrecon etc. "],["main-advantages.html", "5.3 Main advantages", " 5.3 Main advantages Fast prototyping You can quickly write a small pipeline that can be expanded incrementally. Each task is independent and can be easily added to other ones. You can reuse your scripts and tools without rewriting / adapting them. - Reproducibility Nextflow supports Docker and Singularity containers technology. Their use will make the pipelines reproducible in any unix environment. Nextflow is integrated with GitHub code sharing platform so you can call directly a specific version of pipeline from a repository, download it and use it on the fly. Portable Nextflow can be executed on multiple platforms without modifiying the code. It supports several schedulers such as SGE, LSF, SLURM, PBS and HTCondor and cloud platforms like Kubernetes, Amazon AWS and Google Cloud. Scalability Nextflow is based on the dataflow programming model which simplifies writing complex pipelines. The tool takes care of parallelizing the processes without additional written code. The resulting applications are inherently parallel and can scale-up or scale-out, transparently, without having to adapt to a specific platform architecture. Resumable, thanks to continuous checkpoints All the intermediate results produced during the pipeline execution are automatically tracked. For each process a temporary folder is created and is cached (or not) once resuming an execution. "],["workflow-structure.html", "5.4 Workflow structure", " 5.4 Workflow structure The workflows can be repesented as graphs where the nodes are the processes and the edges are the channels. The processes are block of code that can be executed such as scripts or programs, while the channels are asynchronous queue able to connect processess among them via input / output. Each process is independent from the other and can be run in parallel depending on the number of elements in a channel. In the previous example the processes A, B and C can be run in parallel and only at their end the process D is triggered. "],["installation.html", "5.5 Installation", " 5.5 Installation We need at least Java version 8. java -version Then we can install it by typing: curl -s https://get.nextflow.io | bash This will create an executable called nextflow that can be moved to /usr/local/bin or where we prefer. We can finally test using: ./nextflow run hello N E X T F L O W ~ version 20.07.1 Pulling nextflow-io/hello ... downloaded from https://github.com/nextflow-io/hello.git Launching `nextflow-io/hello` [peaceful_brahmagupta] - revision: 96eb04d6a4 [master] executor &gt; local (4) [d7/d053b5] process &gt; sayHello (4) [100%] 4 of 4 ✔ Ciao world! Bonjour world! Hello world! Hola world! This command download from the a github repository and it runs the test script hello. "],["channels-and-operators.html", "5.6 Channels and Operators", " 5.6 Channels and Operators A queue channel is a non-blocking unidirectional FIFO (First In, First Out) queue which connects two processes or operators. A value channel a.k.a. singleton channel is bound to a single value and it can be read unlimited times without consuming its content. An operator is a method that reshape or connect different channels applying some rules. We can make a very simple Nextflow script by writing this code in a file (test.nf): #!/usr/bin/env nextflow // This is a comment /* * This is a block of comments */ // This is needed for activating the new DLS2 nextflow.enable.dsl=2 //Let&#39;s create a channel from string values str = Channel.from(&#39;hello&#39;, &#39;hola&#39;, &#39;bonjour&#39;) /* * Let&#39;s print that channel using the operator view() * https://www.nextflow.io/docs/latest/operator.html#view */ str.view() Then to execute it you can just run: $nextflow run test.nf N E X T F L O W ~ version 20.07.1 Launching `test.nf` [agitated_avogadro] - revision: 61a595c5bf hello hola bonjour As you can see the Channel is just a collection of values, but it can be also a collection of file paths. Let’s create three files: touch aa.txt bb.txt cc.txt And let’s create another script: #!/usr/bin/env nextflow nextflow.enable.dsl=2 /* * Let&#39;s create the channel `my_files` * using the method fromPath */ Channel .fromPath( &quot;*.txt&quot; ) .set {my_fyles} my_fyles.view() nextflow run test2.nf N E X T F L O W ~ version 20.07.1 Launching `test2.nf` [condescending_hugle] - revision: f513c0fac3 /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/aa.txt /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/bb.txt /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/cc.txt Once executed we can see that a folder named work is generated. In that folder Nextflow will store the intermediate files generated by the processes. "],["exercise-1.html", "5.7 EXERCISE 1", " 5.7 EXERCISE 1 Let’s create couple of files (like paired end reads) and let’s try to read it as they were a tuple. First create a couple of files: touch aaa_1.txt aaa_2.txt See here fromFilePairs. Answer #!/usr/bin/env nextflow nextflow.enable.dsl=2 /* * Let&#39;s create the channel `my_files` * using the method fromFilePairs */ Channel .fromFilePairs( &quot;aaa_{1,2}.txt&quot; ) .set {my_fyles} my_fyles.view() Try to reshape the input channel using different operatoes by generating: A single emission A channel with each possible file combination A tuple with a custom id, i.e. something like [“id”, [“aa.txt”, “bb.txt”, “cc.txt”]] See here the list of Operators available. Answer #!/usr/bin/env nextflow nextflow.enable.dsl=2 Channel .fromPath( &quot;*.txt&quot;).set {my_fyles} my_fyles.collect().view() my_fyles.combine(my_fyles).view() my_fyles.collect().map{ [&quot;id&quot;, it] }.view() my_fyles.view() "],["processes.html", "5.8 Processes", " 5.8 Processes Let’s add now a process to our previous script test.nf: #!/usr/bin/env nextflow nextflow.enable.dsl=2 str = Channel.from(&#39;hello&#39;, &#39;hola&#39;, &#39;bonjour&#39;) /* * Creates a process which receive an input channel containing values * Each value emitted by the channel triggers the execution * of the process. The process stdout is caputured and send over * the another channel. */ process printHello { tag { str_in } // this is for displaying the content of `str_in` in the log file input: val str_in output: stdout script: &quot;&quot;&quot; echo ${str_in} in Italian is ciao &quot;&quot;&quot; } The process can be seen as a function and is composed by: - An input part where the input channels are defined - An output part where we indicates what to store as a result that will be sent to other processes or published as final result - A script part where we have the block of code to be executed with the data from the input channel and will produce the output for the ouput channel. You can run any kind of code / command line there, it is language agnostic. You can have some trouble with escaping some characters, in that case is better to wrap your code in a file and call it as a program. Before the input you can indicate a tag that will be reported in the log. This is quite useful for logging / debugging. "],["workflow.html", "5.9 Workflow", " 5.9 Workflow The code as it is will not produce anything, because you need another part that actually call the process and connect it to the input channel. This part is called a workflow. So let’s change our code: #!/usr/bin/env nextflow nextflow.enable.dsl=2 str = Channel.from(&#39;hello&#39;, &#39;hola&#39;, &#39;bonjour&#39;) process printHello { tag { str_in } input: val str_in output: stdout script: &quot;&quot;&quot; echo ${str_in} in Italian is ciao &quot;&quot;&quot; } /* * A workflow consist of a number of invocations of processes * where they are fed with the expected input channels * as they were cutom functions. You can only invoke a process once per workflow. */ workflow { result = printHello(str) result.view() } We can run the script this time sending the execution in background and sending the log to a file. nextflow run test.nf -bg &gt; log.txt "],["nextflow-log.html", "5.10 Nextflow log", " 5.10 Nextflow log Let’s inspect now the log file: cat log.txt N E X T F L O W ~ version 20.07.1 Launching `test.nf` [high_fermat] - revision: b129d66e57 [6a/2dfcaf] Submitted process &gt; printHello (hola) [24/a286da] Submitted process &gt; printHello (hello) [04/e733db] Submitted process &gt; printHello (bonjour) hola in Italian is ciao hello in Italian is ciao bonjour in Italian is ciao The tag allows us to see that the process printHello was launched three times on hola, hello and bonjour values contained in the input channel. In each row there is a code before: [6a/2dfcaf] Submitted process &gt; printHello (hola) This code indicates also the path in which is process is “isolated” and where you have the temporary files. Let’s have a look: echo work/6a/2dfcaf* work/6a/2dfcafc01350f475c60b2696047a87 ls -alht work/6a/2dfcaf* ls -alht work/6a/2dfcaf* total 40 -rw-r--r-- 1 lcozzuto staff 1B Oct 7 13:39 .exitcode drwxr-xr-x 9 lcozzuto staff 288B Oct 7 13:39 . -rw-r--r-- 1 lcozzuto staff 24B Oct 7 13:39 .command.log -rw-r--r-- 1 lcozzuto staff 24B Oct 7 13:39 .command.out -rw-r--r-- 1 lcozzuto staff 0B Oct 7 13:39 .command.err -rw-r--r-- 1 lcozzuto staff 0B Oct 7 13:39 .command.begin -rw-r--r-- 1 lcozzuto staff 45B Oct 7 13:39 .command.sh -rw-r--r-- 1 lcozzuto staff 2.5K Oct 7 13:39 .command.run drwxr-xr-x 3 lcozzuto staff 96B Oct 7 13:39 .. You see a lot of “hidden” files: - .exitcode, contains 0 if everything is ok, another value if there was a problem. - .command.log, contains the log of the command execution. Often is identical to .command.out - .command.out, contains the standard output of the command execution - .command.err, contains the standard error of the command execution - .command.begin, contains what has to be executed before .command.sh - .command.sh, contains the block of code indicated in the process - .command.run, contains the code made by nextflow for the execution of .command.sh and contains environmental variables, eventual invocations of linux containers etc. For instance the content of .command.sh is: cat work/6a/2dfcaf*/.command.sh #!/bin/bash -ue echo hola in Italian is ciao And the content of .command.out is cat work/6a/2dfcaf*/.command.out hola in Italian is ciao You can also name workflows as they were collections of processes. For instance we can write: #!/usr/bin/env nextflow nextflow.enable.dsl=2 str = Channel.from(&#39;hello&#39;, &#39;hola&#39;, &#39;bonjour&#39;) process printHello { tag { str_in } input: val str_in output: stdout script: &quot;&quot;&quot; echo ${str_in} in Italian is ciao &quot;&quot;&quot; } /* * A workflow can be named as a function and receive an input using the take keyword */ workflow first_pipeline { take: str_input main: printHello(str_input).view() } /* * You can re-use the previous processes an combine as you prefer */ workflow second_pipeline { take: str_input main: printHello(str_input.collect()).view() } /* * You can then invoke the different named workflows in this way * passing the same input channel `str` to both */ workflow { first_pipeline(str) second_pipeline(str) } You can see that with the previous code you can execute two workflows containing the same process. We add the collect operator to the second workflow that collects the output from different executions and return the resulting list as a sole emission. Let’s run the code: nextflow run test.nf -bg &gt; log2 cat log2 cat log2 N E X T F L O W ~ version 20.07.1 Launching `test.nf` [irreverent_davinci] - revision: 25a5511d1d [de/105b97] Submitted process &gt; first_pipeline:printHello (hello) [ba/051c23] Submitted process &gt; first_pipeline:printHello (bonjour) [1f/9b41b2] Submitted process &gt; second_pipeline:printHello (hello) [8d/270d93] Submitted process &gt; first_pipeline:printHello (hola) [18/7b84c3] Submitted process &gt; second_pipeline:printHello (hola) hello in Italian is ciao bonjour in Italian is ciao [0f/f78baf] Submitted process &gt; second_pipeline:printHello (bonjour) hola in Italian is ciao [&#39;hello in Italian is ciao\\n&#39;, &#39;hola in Italian is ciao\\n&#39;, &#39;bonjour in Italian is ciao\\n&#39;] "],["exercise-2.html", "5.11 EXERCISE 2", " 5.11 EXERCISE 2 Change the pipeline for producing files instead of standard output. You need to specify within the workflow what to output using emit keyword. See here emit Tip: you can chose the elements from a list using the positional keys (i.e. list[0], list[1], etc…) Answer #!/usr/bin/env nextflow nextflow.enable.dsl=2 str = Channel.from(&#39;hello&#39;, &#39;hola&#39;, &#39;bonjour&#39;) process printHello { tag { str_in } input: val str_in output: path(&quot;${str_in[0]}.txt&quot;) script: &quot;&quot;&quot; echo ${str_in} in Italian is ciao &gt; ${str_in[0]}.txt &quot;&quot;&quot; } /* * A workflow can be named as a function and receive an input using the take keyword */ workflow first_pipeline { take: str_input main: out = printHello(str_input) emit: out } /* * You can re-use the previous processes an combine as you prefer */ workflow second_pipeline { take: str_input main: out = printHello(str_input).collect() emit: out } /* * You can then invoke the different named workflows in this way * passing the same input channel `str` to both */ workflow { out1 = first_pipeline(str) out2 = second_pipeline(str) } Change the pipeline for producing files instead of standard output. You need to specify within the workflow what to output using emit keyword. See here emit "],["more-complex-scripts.html", "5.12 More complex scripts", " 5.12 More complex scripts We can feed the channel that is generated by a process to another process in the workflow definition. In this way we have a proper pipeline. #!/usr/bin/env nextflow nextflow.enable.dsl=2 // this can be overridden by using --inputfile OTHERFILENAME params.inputfile = &quot;$baseDir/testdata/test.fa&quot; // the &quot;file method&quot; returns a file system object given a file path string sequences_file = file(params.inputfile) // check if the file exists if( !sequences_file.exists() ) exit 1, &quot;Missing genome file: ${genome_file}&quot; /* * Process 1 for splitting a fasta file in multiple files */ process splitSequences { input: path sequencesFile output: path (&#39;seq_*&#39;) // simple awk command script: &quot;&quot;&quot; awk &#39;/^&gt;/{f=&quot;seq_&quot;++d} {print &gt; f}&#39; &lt; ${sequencesFile} &quot;&quot;&quot; } /* * Process 2 for reversing the sequences */ process reverseSequence { tag &quot;$seq&quot; publishDir &quot;output&quot; input: path seq output: path &quot;all.rev&quot; script: &quot;&quot;&quot; cat ${seq} | awk &#39;{if (\\$1~&quot;&gt;&quot;) {print \\$0} else system(&quot;echo &quot; \\$0 &quot; |rev&quot;)}&#39; &gt; all.rev &quot;&quot;&quot; } workflow { splitted_seq = splitSequences(sequences_file) // Here you have the output channel as a collection splitted_seq.view() // Here you have the same channel reshaped to send separately each value splitted_seq.flatten().view() // DLS2 allows you to reuse the channels! In past you had to create many identical // channels for sending the same kind of data to different processes rev_single_seq = reverseSequence(splitted_seq) } Here we have two simple processes: the former splits the input fasta file in single sequences the latter is able to reverse the position of the sequences. The input path is fed as a parameter using the script parameters ${seq} params.inputfile In this way this value can be overridden when calling the script in this way: nextflow run test.nf --inputfile another_input.fa The workflow part connects the two processes so that the output of the first process is fed to the second one. During the execution Nextflow creates a number of temporary folders and this time will make also a soft link to the original input file. Then it will store output files locally. The output file is then linked in other folders for being used as input from other processes. In this way there are no clashes and each process is nicely isolated from the others. nextflow run test.nf -bg N E X T F L O W ~ version 20.07.1 Launching `test.nf` [sad_newton] - revision: 82e66714e4 [09/53e071] Submitted process &gt; splitSequences [/home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/work/09/53e071d286ed66f4020869c8977b59/seq_1, /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/work/09/53e071d286ed66f4020869c8977b59/seq_2, /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/work/09/53e071d286ed66f4020869c8977b59/seq_3] /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/work/09/53e071d286ed66f4020869c8977b59/seq_1 /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/work/09/53e071d286ed66f4020869c8977b59/seq_2 /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/work/09/53e071d286ed66f4020869c8977b59/seq_3 [fe/0a8640] Submitted process &gt; reverseSequence ([seq_1, seq_2, seq_3]) if we inspect the content of work/09/53e071* generated by the process splitSequences ls -l work/09/53e071* total 24 -rw-r--r-- 1 lcozzuto staff 29 Oct 8 19:16 seq_1 -rw-r--r-- 1 lcozzuto staff 33 Oct 8 19:16 seq_2 -rw-r--r-- 1 lcozzuto staff 27 Oct 8 19:16 seq_3 lrwxr-xr-x 1 lcozzuto staff 69 Oct 8 19:16 test.fa -&gt; /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/testdata/test.fa we have the file test.fa that is a soft link to the orinal input. And inspecting work/fe/0a8640* that is generated by the process reverseSequence reveals that the files generated by splitSequences are now linked as input. ls -l work/fe/0a8640* total 8 -rw-r--r-- 1 lcozzuto staff 89 Oct 8 19:16 all.rev lrwxr-xr-x 1 lcozzuto staff 97 Oct 8 19:16 seq_1 -&gt; /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/work/09/53e071d286ed66f4020869c8977b59/seq_1 lrwxr-xr-x 1 lcozzuto staff 97 Oct 8 19:16 seq_2 -&gt; /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/work/09/53e071d286ed66f4020869c8977b59/seq_2 lrwxr-xr-x 1 lcozzuto staff 97 Oct 8 19:16 seq_3 -&gt; /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/work/09/53e071d286ed66f4020869c8977b59/seq_3 IMPORTANT: Nextflow will randomly generate temporary folders so they will be named differently in your execution!!! At this point we can make two different workflows so that we show how the new DSL allows reusing of the code. #!/usr/bin/env nextflow nextflow.enable.dsl=2 // this can be overridden by using --inputfile OTHERFILENAME params.inputfile = &quot;$baseDir/testdata/test.fa&quot; // the &quot;file method&quot; returns a file system object given a file path string sequences_file = file(params.inputfile) // check if the file exists if( !sequences_file.exists() ) exit 1, &quot;Missing genome file: ${genome_file}&quot; /* * Process 1 for splitting a fasta file in multiple files */ process splitSequences { input: path sequencesFile output: path (&#39;seq_*&#39;) // simple awk command script: &quot;&quot;&quot; awk &#39;/^&gt;/{f=&quot;seq_&quot;++d} {print &gt; f}&#39; &lt; ${sequencesFile} &quot;&quot;&quot; } /* * Process 2 for reversing the sequences */ process reverseSequence { tag &quot;$seq&quot; input: path seq output: path &quot;all.rev&quot; script: &quot;&quot;&quot; cat ${seq} | awk &#39;{if (\\$1~&quot;&gt;&quot;) {print \\$0} else system(&quot;echo &quot; \\$0 &quot; |rev&quot;)}&#39; &gt; all.rev &quot;&quot;&quot; } workflow flow1 { take: sequences main: splitted_seq = splitSequences(sequences) rev_single_seq = reverseSequence(splitted_seq) } workflow flow2 { take: sequences main: splitted_seq = splitSequences(sequences).flatten() rev_single_seq = reverseSequence(splitted_seq) } workflow { flow1(sequences_file) flow2(sequences_file) } The first workflow will just run like the previous script, while the second will “flatten” the output of the first process and will launch the second process on each single sequence. The reverseSequence processes of the second workflow will run in parallel if you have enough processors or you are in a cluster environment with a scheduler supported by Nextflow. nextflow run test1.nf -bg C02WX1XFHV2Q:nextflow lcozzuto$ N E X T F L O W ~ version 20.07.1 Launching `test.nf` [insane_plateau] - revision: d33befe154 [bd/f4e9a6] Submitted process &gt; flow1:splitSequences [37/d790ab] Submitted process &gt; flow2:splitSequences [33/a6fc72] Submitted process &gt; flow1:reverseSequence ([seq_1, seq_2, seq_3]) [87/54bfe8] Submitted process &gt; flow2:reverseSequence (seq_2) [45/86dd83] Submitted process &gt; flow2:reverseSequence (seq_1) [93/c7b1c6] Submitted process &gt; flow2:reverseSequence (seq_3) "],["directives.html", "5.13 Directives", " 5.13 Directives The directives are declarations blocks that can provide optional settings to a process. For instance they can affect the way a process stage in and out the input and ouptut files (stageInMode and stageOutMode or they can indicate which files has to be considered a final result and to be published in which folder (publishDir). We can add the directive publishDir to our previous example: /* * Simple reverse the sequences */ process reverseSequence { tag &quot;$seq&quot; // during the execution prints the indicated variable for follow-up publishDir &quot;output&quot; input: path seq output: path &quot;all.rev&quot; script: &quot;&quot;&quot; cat ${seq} | awk &#39;{if (\\$1~&quot;&gt;&quot;) {print \\$0} else system(&quot;echo &quot; \\$0 &quot; |rev&quot;)}&#39; &gt; all.rev &quot;&quot;&quot; } We can also use storeDir in case we want to have a permanent cache. Basically the process is executed only if the output files do not exist in the folder specified by storeDir. When the output files exist the process execution is skipped and these files are used as the actual process result. As an example this can be useful in case we don’t want to generate indexes each time and we prefer to reuse them. We can also indicate what to do in case a process fail. The default is to stop the pipeline and to raise an error. But we can also skip the process using this directive: errorStrategy &#39;ignore&#39; or retry a number of times changing something like the memory available or the maximum execution time. This time we need a number of directives: memory { 1.GB * task.attempt } time { 1.hour * task.attempt } errorStrategy &#39;retry&#39; maxRetries 3 "],["resuming-your-pipeline.html", "5.14 Resuming your pipeline", " 5.14 Resuming your pipeline You can resume the execution after the code modification using the parameter -resume. Nextflow is smart enough to cache the execution since input and output were not changed. nextflow run test.nf -bg -resume N E X T F L O W ~ version 20.07.1 Launching `test.nf` [determined_celsius] - revision: eaf5b4d673 [bd/f4e9a6] Cached process &gt; flow1:splitSequences [37/d790ab] Cached process &gt; flow2:splitSequences [93/c7b1c6] Cached process &gt; flow2:reverseSequence (seq_3) [45/86dd83] Cached process &gt; flow2:reverseSequence (seq_1) [87/54bfe8] Cached process &gt; flow2:reverseSequence (seq_2) [33/a6fc72] Cached process &gt; flow1:reverseSequence ([seq_1, seq_2, seq_3]) /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/work/33/a6fc72786d042cacf733034d501691/all.rev IMPORTANT: Nextflow parameters are with one hyphen (-resume) while pipeline parameters are with two (–inputfile) Sometimes you might want to resume a previous run of your pipeline. For doing so you need to extract the job id of that run. You can do this by using the command nextflow log nextflow log TIMESTAMP DURATION RUN NAME STATUS REVISION ID SESSION ID COMMAND 2020-10-06 14:49:09 2s agitated_avogadro OK 61a595c5bf 4a7a8a4b-9bdb-4b15-9cc6-1b2cabe9a938 nextflow run test.nf 2020-10-08 19:14:38 2.8s sick_edison OK 82e66714e4 4fabb863-2038-47b4-bac0-19e71f93f284 nextflow run test.nf -bg 2020-10-08 19:16:03 3s sad_newton OK 82e66714e4 2d13e9f8-1ba6-422d-9087-5c6c9731a795 nextflow run test.nf -bg 2020-10-08 19:30:59 2.3s disturbed_wozniak OK d33befe154 0a19b60d-d5fe-4a26-9e01-7a63d0a1d300 nextflow run test.nf -bg 2020-10-08 19:35:52 2.5s insane_plateau OK d33befe154 b359f32c-254f-4271-95bb-6a91b281dc6d nextflow run test.nf -bg 2020-10-08 19:56:30 2.8s determined_celsius OK eaf5b4d673 b359f32c-254f-4271-95bb-6a91b281dc6d nextflow run test.nf -bg -resume You can then resume the state of your execution using the SESSION ID in this way: nextflow run -resume 0a19b60d-d5fe-4a26-9e01-7a63d0a1d300 test.nf Nextflow’s cache can be disabled for a specific process adding setting the directive cache to false. You can also choose three caching methods: cache = true // (default) Cache keys are created indexing input files meta-data information (name, size and last update timestamp attributes). cache = &#39;deep&#39; // Cache keys are created indexing input files content. cache = &#39;lenient&#39; // (Best in HPC and shared file systems) Cache keys are created indexing input files path and size attributes IMPORTANT On some shared file systems you might have inconsistent files timestamps. So cache lenient prevent you from unwanted restarting of cached processes. "],["exercise-3.html", "5.15 EXERCISE 3", " 5.15 EXERCISE 3 Try to make the previous pipeline resilient to the failing of a process and store the results in order to skip the process execution when launched again. Answer workflow flow1 { take: sequences main: splitSequences(sequences) | reverseSequence | view() } Write the first workflow using pipes. Nextflow DLS2 allows you to use pipes for connecting channels via input / output. See documentation here: https://www.nextflow.io/docs/latest/dsl2.html#pipes Answer workflow flow1 { take: sequences main: splitSequences(sequences) | reverseSequence | view() } "],["nextflow-day-2.html", "Part 6 Nextflow day 2", " Part 6 Nextflow day 2 During this day we will make more complex pipelines and separate the main code from the configuration. Then we will focus on the reuse of the code and on how to share your code. "],["decoupling-resources-parameters-and-nextflow-script.html", "6.1 Decoupling resources, parameters and nextflow script", " 6.1 Decoupling resources, parameters and nextflow script When you make a complex pipelines you might want to keep separated the definition of resources needed, the default parameters and the main script. You can achieve this by two additional files: nextflow.config params.config The nextflow.config file allows to indicate the resources needed for each class of processes. You can label your processes to make a link with the definitions in the nextflow.config file. This is an example of a nextflow.config file: includeConfig &quot;$baseDir/params.config&quot; process { memory=&#39;0.6G&#39; cpus=&#39;1&#39; time=&#39;6h&#39; withLabel: &#39;onecpu&#39; { memory=&#39;0.6G&#39; cpus=&#39;1&#39; } } process.container = &#39;biocorecrg/c4lwg-2018:latest&#39; singularity.cacheDir = &quot;$baseDir/singularity&quot; The first row indicates to use the information stored in the params.config file (described later). Then we have the definition of the default resources for a process: process { memory=&#39;0.6G&#39; cpus=&#39;1&#39; time=&#39;6h&#39; ... Then we have the resources needed for a class of processes in particular labeled with bigmem withLabel: &#39;bigmem&#39; { memory=&#39;0.7G&#39; cpus=&#39;1&#39; } If we have a look at the process fastQC within the test2.nf file, we can see the use of the label. /* * Process 1. Run FastQC on raw data. */ process fastQC { publishDir fastqcOutputFolder tag { reads } label &#39;bigmem&#39; input: path reads ... The latest two rows of the config file indicates which container needs to be used. In this case, it assumes has to be pulled from DockerHub. In case you want to use singularity you can indicate where to store the local image by using the singularity.cacheDir setting process.container = &#39;biocorecrg/c4lwg-2018:latest&#39; singularity.cacheDir = &quot;$baseDir/singularity&quot; Let’s now launch the script test2.nf cd test2; nextflow run test2.nf N E X T F L O W ~ version 20.07.1 Launching `test2.nf` [distracted_edison] - revision: e3a80b15a2 BIOCORE@CRG - N F TESTPIPE ~ version 1.0 ============================================= reads : /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/test2/../testdata/*.fastq.gz executor &gt; local (2) [df/2c45f2] process &gt; fastQC (B7_input_s_chr19.fastq.gz) [ 0%] 0 of 2 [- ] process &gt; multiQC - Error executing process &gt; &#39;fastQC (B7_H3K4me1_s_chr19.fastq.gz)&#39; Caused by: Process `fastQC (B7_H3K4me1_s_chr19.fastq.gz)` terminated with an error exit status (127) Command executed: fastqc B7_H3K4me1_s_chr19.fastq.gz Command exit status: 127 executor &gt; local (2) [df/2c45f2] process &gt; fastQC (B7_input_s_chr19.fastq.gz) [100%] 2 of 2, failed: 2 ✘ [- ] process &gt; multiQC - Error executing process &gt; &#39;fastQC (B7_H3K4me1_s_chr19.fastq.gz)&#39; Caused by: Process `fastQC (B7_H3K4me1_s_chr19.fastq.gz)` terminated with an error exit status (127) Command executed: fastqc B7_H3K4me1_s_chr19.fastq.gz Command exit status: 127 Command output: (empty) Command error: .command.sh: line 2: fastqc: command not found Work dir: /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/test2/work/c5/18e76b2e6ffd64aac2b52e69bedef3 Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line We will get a number of errors since no executable is found in our environment / path. This because they are stored in our docker image! So we can launch it this time with the -with-docker parameter. nextflow run test2.nf -with-docker nextflow run test2.nf -with-docker N E X T F L O W ~ version 20.07.1 Launching `test2.nf` [boring_hamilton] - revision: e3a80b15a2 BIOCORE@CRG - N F TESTPIPE ~ version 1.0 ============================================= reads : /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/test2/../testdata/*.fastq.gz executor &gt; local (3) [22/b437be] process &gt; fastQC (B7_H3K4me1_s_chr19.fastq.gz) [100%] 2 of 2 ✔ [1a/cfe63b] process &gt; multiQC [ 0%] 0 of 1 executor &gt; local (3) [22/b437be] process &gt; fastQC (B7_H3K4me1_s_chr19.fastq.gz) [100%] 2 of 2 ✔ [1a/cfe63b] process &gt; multiQC [100%] 1 of 1 ✔ This time it worked beautifully since Nextflow used the image indicated within the nextflow.config file that contains our executables. Now we can have a look at the params.config file params { reads = &quot;$baseDir/../testdata/*.fastq.gz&quot; email = &quot;myemail@google.com&quot; } As you can see we indicates the pipeline parameters that can be overridden by using –reads and –email. This is not mandatory but I found quite useful to modify this file instead of using very long command lines with tons of –something. Now, let’s have a look at the folders generated by the pipeline. ls work/2a/22e3df887b1b5ac8af4f9cd0d88ac5/ total 0 drwxrwxr-x 3 ec2-user ec2-user 26 Apr 23 13:52 . drwxr-xr-x 2 root root 136 Apr 23 13:51 multiqc_data drwxrwxr-x 3 ec2-user ec2-user 44 Apr 23 13:51 .. We observe that Docker runs as “root”. This can be problematic and generates security issues. To avoid this we can add this line of code within the process section of the config file: containerOptions = { workflow.containerEngine == &quot;docker&quot; ? &#39;-u $(id -u):$(id -g)&#39;: null} This will tell to Nextflow that if is running with docker, this has to produce files that belong to your user and not to root. 6.1.1 Publishing final results After running the script you see two new folders named ouptut_fastqc and ouptut_fastqc that contain the result of the pipeline. We can indicate which process and which output can be considered the final output of the pipeline by using the publishDir directive that has to be specified at the beginning of a process. In our pipeline we define these folders here: /* * Defining the output folders. */ fastqcOutputFolder = &quot;ouptut_fastqc&quot; multiqcOutputFolder = &quot;ouptut_multiQC&quot; [...] /* * Process 1. Run FastQC on raw data. A process is the element for executing scripts / programs etc. */ process fastQC { publishDir fastqcOutputFolder // where (and whether) to publish the results [...] /* * Process 2. Run multiQC on fastQC results */ process multiQC { publishDir multiqcOutputFolder, mode: &#39;copy&#39; // this time do not link but copy the output file You can see that the default mode to publish the results in Nextflow is soft linking. You can change this behaviour by specifying the mode as indicated in the multiQC process. IMPORTANT: You can also “move” the results but this is not suggested for files that will be needed for other processes. This will likely disrupt your pipeline. "],["adding-a-help-section-for-the-whole-pipeline.html", "6.2 Adding a help section for the whole pipeline", " 6.2 Adding a help section for the whole pipeline In this example we also describe another good practice: the use of the –help parameter. At the beginning of the pipeline we can write: params.help = false // this prevents a warning of undefined parameter // this prints the input parameters log.info &quot;&quot;&quot; BIOCORE@CRG - N F TESTPIPE ~ version ${version} ============================================= reads : ${params.reads} &quot;&quot;&quot; // this prints the help in case you use --help parameter in the command line and it stops the pipeline if (params.help) { log.info &#39;This is the Biocore\\&#39;s NF test pipeline&#39; log.info &#39;Enjoy!&#39; log.info &#39;\\n&#39; exit 1 } so launching the pipeline with –help will show you just the parameters and the help. nextflow run test2.nf --help N E X T F L O W ~ version 20.07.1 Launching `test2.nf` [mad_elion] - revision: e3a80b15a2 BIOCORE@CRG - N F TESTPIPE ~ version 1.0 ============================================= reads : /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/test2/../testdata/*.fastq.gz This is the Biocore&#39;s NF test pipeline Enjoy! "],["using-singularity.html", "6.3 Using Singularity", " 6.3 Using Singularity We recommend to use Singularity instead of docker in HPC environments. This can be done by just using the Nextflow parameter -with-singularity and without touching the code. Nextflow will take care of pulling, converting and storing the image for you. This will be done just once and then nextflow will use the stored image for further executions. Within the AWS main node both Docker and singularity are available. Within the AWS batch we only have docker. nextflow run test2.nf -with-singularity -bg &gt; log tail -f log N E X T F L O W ~ version 20.10.0 Launching `test2.nf` [soggy_miescher] - revision: 5a0a513d38 BIOCORE@CRG - N F TESTPIPE ~ version 1.0 ============================================= reads : /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/test2/../../testdata/*.fastq.gz Pulling Singularity image docker://biocorecrg/c4lwg-2018:latest [cache /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/test2/singularity/biocorecrg-c4lwg-2018-latest.img] [da/eb7564] Submitted process &gt; fastQC (B7_H3K4me1_s_chr19.fastq.gz) [f6/32dc41] Submitted process &gt; fastQC (B7_input_s_chr19.fastq.gz) ... We can then inspect the presence of the singularity image inside the folder singularity. ls singularity/ biocorecrg-c4lwg-2018-latest.img We can then reuse this image if we want to execute the code exactly in the same way as in the pipeline but outside the pipeline. Sometimes we can be interested in launching just one job, because it failed or for just making a test. We can go to the corresponding temporary folder, as an example let’s go to one of the fastQC temporary folder: cd work/da/eb7564*/ Inspecting the .command.run file shows us this piece of code: ... nxf_launch() { set +u; env - PATH=&quot;$PATH&quot; SINGULARITYENV_TMP=&quot;$TMP&quot; SINGULARITYENV_TMPDIR=&quot;$TMPDIR&quot; singularity exec /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/test2/singularity/biocorecrg-c4lwg-2018-latest.img /bin/bash -c &quot;cd $PWD; /bin/bash -ue /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/test2/work/da/eb756433aa0881d25b20afb5b1366e/.command.sh&quot; } ... This means that Nextflow is running the code by using the singularity exec command. Then we can launch the following command: bash .command.run Started analysis of B7_H3K4me1_s_chr19.fastq.gz Approx 5% complete for B7_H3K4me1_s_chr19.fastq.gz Approx 10% complete for B7_H3K4me1_s_chr19.fastq.gz Approx 15% complete for B7_H3K4me1_s_chr19.fastq.gz Approx 20% complete for B7_H3K4me1_s_chr19.fastq.gz Approx 25% complete for B7_H3K4me1_s_chr19.fastq.gz Approx 30% complete for B7_H3K4me1_s_chr19.fastq.gz Approx 35% complete for B7_H3K4me1_s_chr19.fastq.gz Approx 40% complete for B7_H3K4me1_s_chr19.fastq.gz Approx 45% complete for B7_H3K4me1_s_chr19.fastq.gz Approx 50% complete for B7_H3K4me1_s_chr19.fastq.gz Approx 55% complete for B7_H3K4me1_s_chr19.fastq.gz Approx 60% complete for B7_H3K4me1_s_chr19.fastq.gz ... In this way you are doing the same execution done by nextflow using the local machine. In case you are submitting a job to a HPC you need to use the corresponding program, for instance qsub. qsub .command.run "],["adding-more-steps.html", "6.4 Adding more steps", " 6.4 Adding more steps We can make pipelines incrementally by adding more and more processes. Nextflow will take care of the dependencies between the input / output and of the parallelization. Within the test3 folder we have two more steps: the reference indexing and the read alignments with bowtie (http://bowtie-bio.sourceforge.net/index.shtml). So we add a new input for the reference sequence: log.info &quot;&quot;&quot; BIOCORE@CRG - N F TESTPIPE ~ version ${version} ============================================= reads : ${params.reads} reference : ${params.reference} &quot;&quot;&quot; reference = file(params.reference) We generate in this way a singleton channel called reference which content is never consumed and can be indefinitely used. We add two more processes. The first one is for the indexing of the reference: /* * Process 2. Bowtie index */ process bowtieIdx { tag { ref } input: path ref output: tuple val(&quot;${ref}&quot;), path (&quot;${ref}*.ebwt&quot;) script: &quot;&quot;&quot; gunzip -c ${ref} &gt; reference.fa bowtie-build reference.fa ${ref} rm reference.fa &quot;&quot;&quot; } Since bowtie indexing requires unzipped reference we unzip it then we build the reference and remove the unzipped file afterwards. The output channel generated is organized as a tuple, i.e. a list of elements. The first element of the list is the name of the index as a value, the second is a list of files constituting the index. The former is needed for building the command line of the alignment step, the latter are the files needed for the alignment. The second process is the alignment step: /* * Process 3. Bowtie alignment */ process bowtieAln { publishDir alnOutputFolder, pattern: &#39;*.sam&#39; tag { reads } label &#39;twocpus&#39; input: tuple val(refname), path (ref_files) path reads output: path &quot;${reads}.sam&quot;, emit: samples_sam path &quot;${reads}.log&quot;, emit: samples_log script: &quot;&quot;&quot; bowtie -p ${task.cpus} ${refname} -q ${reads} -S &gt; ${reads}.sam 2&gt; ${reads}.log &quot;&quot;&quot; } As you see there are two different input channels: the index one and the reads. The index name specified by refname is used for building the command line while the index files, indicated by ref_files, are just linked in the current directory by using the path qualifier. We also produced two kind of outputs: the alignments and the logs. The first one is the one we want to keep as a final result. So we specify this using the publishDir pattern parameter. publishDir alnOutputFolder, pattern: &#39;*.sam&#39; The second one will be just passed to the next process for being used by the multiQC process. To distinghuish among them we can assign them different names. output: path &quot;${reads}.sam&quot;, emit: samples_sam path &quot;${reads}.log&quot;, emit: samples_log This section will allow us to connect these outputs directly with other processes when we call them in the workflow section: workflow { fastqc_out = fastQC(reads) bowtie_index = bowtieIdx(reference) bowtieAln(bowtie_index, reads) multiQC(fastqc_out.mix(bowtieAln.out.samples_log).collect()) } So we passed the samples_log output to the multiqc process after mixing it with the output channel from the fastqc process. "],["profiles.html", "6.5 Profiles", " 6.5 Profiles For deploying a pipeline on a cluster environment or a cloud we need to indicate some information on the nextflow.config file. In particular we need to indicate the kind of executor to be used. In the Nextflow framework architecture, the executor indicates which is the batch-queuing system to use to submit jobs to the HPC or to the cloud. The executor is completely abstracted, so you can switch from SGE to SLURM just by changing this parameter in the configuration file. You can group different class of configuration or profiles within a single nextflow.config file. In this way that you can indicate at run time which executor and resources to use for a pipeline execution. Let’s inspect the nextflow.config file in test3 folder. We can look at three different profiles: standard cluster cloud The first one indicates the resources needed for running the pipeline locally. They are quite small since we have little power and CPUs on the test node. profiles { standard { process { containerOptions = { workflow.containerEngine == &quot;docker&quot; ? &#39;-u $(id -u):$(id -g)&#39;: null} executor=&quot;local&quot; memory=&#39;0.6G&#39; cpus=&#39;1&#39; time=&#39;6h&#39; withLabel: &#39;twocpus&#39; { memory=&#39;0.6G&#39; cpus=&#39;1&#39; } } } As you can see we indicate explicitly the local executor. So this will be the default when running the pipeline indicating no profiles. The second one is cluster cluster { process { containerOptions = { workflow.containerEngine == &quot;docker&quot; ? &#39;-u $(id -u):$(id -g)&#39;: null} executor=&quot;SGE&quot; queue=&quot;smallcpus&quot; memory=&#39;1G&#39; cpus=&#39;1&#39; time=&#39;6h&#39; withLabel: &#39;twocpus&#39; { queue=&quot;bigcpus&quot; memory=&#39;4G&#39; cpus=&#39;2&#39; } } } This indicates that the system uses Sun Grid Engine as job scheduler and that we have different queues for small jobs and more intensive ones. "],["deployment-in-the-aws-cloud.html", "6.6 Deployment in the AWS cloud", " 6.6 Deployment in the AWS cloud The final profile is for running the pipeline in the Amazon Cloud, known as Amazon Web Services or AWS. In particular we will use AWS Batch that allows the execution of containerised workloads in the Amazon cloud infrastructure. cloud { workDir = &#39;s3://class-bucket-1/work&#39; aws.region = &#39;eu-central-1&#39; aws.batch.cliPath = &#39;/home/ec2-user/miniconda/bin/aws&#39; process { containerOptions = { workflow.containerEngine == &quot;docker&quot; ? &#39;-u $(id -u):$(id -g)&#39;: null} executor = &#39;awsbatch&#39; queue = &#39;spot&#39; memory=&#39;1G&#39; cpus=&#39;1&#39; time=&#39;6h&#39; withLabel: &#39;twocpus&#39; { memory=&#39;0.6G&#39; cpus=&#39;2&#39; } } } We indicate some aws specific parameters (region and cliPath) and the executor that is awsbatch. Then we indicates that the working directory, that is normally written locally, to be mounted as S3 volume. This is mandatory when running nextflow on the cloud. We can now launch the pipeline indicating -profile cloud nextflow run test3.nf -bg -with-docker -profile cloud &gt; log We can see that there is no more a work folder because is on the AWS cloud and that the output is then copied locally. Sometimes you can find that the Nextflow process itself is very memory intensive and the main node can run out of memory. To avoid this you can reduce the memory needed by setting an environmental variable: export NXF_OPTS=&quot;-Xms50m -Xmx500m&quot; "],["modules-and-re-usage-of-the-code.html", "6.7 Modules and re-usage of the code", " 6.7 Modules and re-usage of the code A great advance of the new DLS2 is to allow the modularization of the code. In particular you can move a named workflow within a module and keep it apart for being accessed from different pipelines. Looking at the test4 folder will give you an idea of what is the code using modules. #!/usr/bin/env nextflow nextflow.enable.dsl=2 /* * Input parameters: read pairs * Params are stored in the params.config file */ version = &quot;1.0&quot; params.help = false // this prints the input parameters log.info &quot;&quot;&quot; BIOCORE@CRG - N F TESTPIPE ~ version ${version} ============================================= reads : ${params.reads} &quot;&quot;&quot; if (params.help) { log.info &#39;This is the Biocore\\&#39;s NF test pipeline&#39; log.info &#39;Enjoy!&#39; log.info &#39;\\n&#39; exit 1 } /* * Defining the output folders. */ fastqcOutputFolder = &quot;ouptut_fastqc&quot; multiqcOutputFolder = &quot;ouptut_multiQC&quot; Channel .fromPath( params.reads ) .ifEmpty { error &quot;Cannot find any reads matching: ${params.reads}&quot; } .set {reads_for_fastqc} /* * Here we include two modules from two files. We also add the parameter OUTPUT to pass them the folders where to publish the results */ include { fastqc } from &quot;${baseDir}/lib/fastqc&quot; addParams(OUTPUT: fastqcOutputFolder) include { multiqc } from &quot;${baseDir}/lib/multiqc&quot; addParams(OUTPUT: multiqcOutputFolder) // The main worflow can directly call the named workflows from the modules workflow { fastqc_out = fastqc(reads_for_fastqc) multiqc(fastqc_out.collect()) } workflow.onComplete { println ( workflow.success ? &quot;\\nDone! Open the following report in your browser --&gt; ${multiqcOutputFolder}/multiqc_report.html\\n&quot; : &quot;Oops .. something went wrong&quot; ) } We now include two modules named fastqc and multiqc from ${baseDir}/lib/fastqc.nf and ${baseDir}/lib/multiqc.nf. Let’s inspect the fastqc module: /* * fastqc module */ params.CONTAINER = &quot;quay.io/biocontainers/fastqc:0.11.9--0&quot; params.OUTPUT = &quot;fastqc_output&quot; process qc { publishDir(params.OUTPUT, mode: &#39;copy&#39;) tag { reads } container params.CONTAINER input: path(reads) output: path(&quot;*_fastqc*&quot;) script: &quot;&quot;&quot; fastqc ${reads} &quot;&quot;&quot; } So we have the modules fastqc that take as input a channel with reads and produces as output the files generated by the fastq program. The module is quite simple: it contains the directive publishDir, the tag, the container to be used and has similar input, output and script session we saw previously. A module can contain its own parameters that can be used for connecting the main script to some variables inside the module. in this example we have the declaration of two parameters that are defined at the beginning: params.CONTAINER = &quot;quay.io/biocontainers/fastqc:0.11.9--0&quot; params.OUTPUT = &quot;fastqc_output&quot; They can be overridden from the main script that is calling the module. - The parameter params.OUTPUT can be used for connecting the definition of the output of this module with the one in the main script. - The parameter params.CONTAINER instead for deciding which image has to be used for this particular module. In this example in our main script we pass only the OUTPUT parameters by writing in this way: include { fastqc } from &quot;${baseDir}/lib/fastqc&quot; addParams(OUTPUT: fastqcOutputFolder) include { multiqc } from &quot;${baseDir}/lib/multiqc&quot; addParams(OUTPUT: multiqcOutputFolder) While we keep the information of the container inside the module for better reproducibility: params.CONTAINER = = &quot;quay.io/biocontainers/fastqc:0.11.9--0&quot; Here you see that we are not using own our image but using directly one provided by biocontainers. Here you can find a list of fastqc images developed and stored by the biocontainers community https://biocontainers.pro/#/tools/fastqc. Let’s have a look now at the multiqc.nf module: /* * multiqc module */ params.CONTAINER = &quot;quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0&quot; params.OUTPUT = &quot;multiqc_output&quot; params.LABEL = &quot;&quot; process multiqc { publishDir(params.OUTPUT, mode: &#39;copy&#39;) container params.CONTAINER label (params.LABEL) input: path (inputfiles) output: path &quot;multiqc_report.html&quot; script: &quot;&quot;&quot; multiqc . &quot;&quot;&quot; } So it is very similar to the fastqc one, we just add an extra parameter for connecting the resources defined in the nextflow.config file and the label indicated in the process. In case we want to use it we would need to change the main code in this way: include { multiqc } from &quot;${baseDir}/lib/multiqc&quot; addParams(OUTPUT: multiqcOutputFolder, LABEL=&quot;onecpu&quot;) This because we specified the label onecpu in out nextflow.config file: includeConfig &quot;$baseDir/params.config&quot; process { container = &#39;biocorecrg/debian-perlbrew-pyenv3-java&#39; memory=&#39;0.6G&#39; cpus=&#39;1&#39; time=&#39;6h&#39; withLabel: &#39;onecpu&#39; { memory=&#39;0.6G&#39; cpus=&#39;1&#39; } } singularity.cacheDir = &quot;$baseDir/singularity&quot; IMPORTANT: you will need to specify a default image when you want to run nextflow -with-docker or -with-singularity and you have containers defined inside the modules "],["exercise-4.html", "6.8 EXERCISE 4", " 6.8 EXERCISE 4 Try to make a module wrapper of the bowtie tool and change the script accordingly as the test3. Answer Solution in the folder test5 "],["reporting-and-graphical-interface.html", "6.9 Reporting and graphical interface", " 6.9 Reporting and graphical interface Nextflow has an embedded function for reporting a number of informations about the resources needed by each job and the timing. Just adding a parameter will give you a nice html report. nextflow run test5.nf -with-docker -bg -with-report &gt; log Nextflow Tower is an open source monitoring and managing platform for Nextflow workflows. There are two versions: - Open source for monitoring of single pipelines - Commercial one for workflow management, monitoring and resource optimisation. We will show the open source one. First of all you need to access the tower.nf website and doing the login using one of the methods. We select the email for receiving the instructions and the token to be used for the pipeline. So we check the email: We then follow the instructions exporting two environmental variables: export TOWER_ACCESS_TOKEN=*******YOUR***TOKEN*****HERE******* export NXF_VER=20.09.1-edge we then launch the pipeline: nextflow run test5.nf -with-singularity -with-tower -bg &gt; log CAPSULE: Downloading dependency io.nextflow:nf-tower:jar:20.09.1-edge CAPSULE: Downloading dependency org.codehaus.groovy:groovy-nio:jar:3.0.5 CAPSULE: Downloading dependency io.nextflow:nextflow:jar:20.09.1-edge CAPSULE: Downloading dependency io.nextflow:nf-httpfs:jar:20.09.1-edge CAPSULE: Downloading dependency org.codehaus.groovy:groovy-json:jar:3.0.5 CAPSULE: Downloading dependency org.codehaus.groovy:groovy:jar:3.0.5 CAPSULE: Downloading dependency io.nextflow:nf-amazon:jar:20.09.1-edge CAPSULE: Downloading dependency org.codehaus.groovy:groovy-templates:jar:3.0.5 CAPSULE: Downloading dependency org.codehaus.groovy:groovy-xml:jar:3.0.5 We finally go to the tower website again: "],["share-nextflow-pipelines-and-good-practices.html", "6.10 Share Nextflow pipelines and good practices", " 6.10 Share Nextflow pipelines and good practices Nextflow supports a number of code sharing platforms: BitBucket, GitHub, and GitLab. This feature allows you to run pipelines by just pointing to an online repository without caring about downloading etc. The default platform is GitHub, so we will use this repository as an example. Let’s create a new repository with a unique name: And then let’s clone it in one of our test folder. Let’s choose the 5. We can get the url path by clicking like on the figure: git clone https://github.com/lucacozzuto/test_course.git Cloning into &#39;test_course&#39;... remote: Enumerating objects: 3, done. remote: Counting objects: 100% (3/3), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), done. So now we have an almost empty folder named test_course. We can just move or copy our files there: cp *.* lib -r test_course/ cd test_course git status # On branch main # Untracked files: # (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) # # lib/ # nextflow.config # params.config # test5.nf nothing added to commit but untracked files present (use &quot;git add&quot; to track) Now we are ready for committing and pushing everything to the online repository. But before we need to rename test5.nf to main.nf. mv test5.nf main.nf git add * git status # On branch main # Changes to be committed: # (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) # # new file: lib/bowtie.nf # new file: lib/fastqc.nf # new file: lib/multiqc.nf # new file: nextflow.config # new file: params.config # new file: main.nf # git commit -m &quot;first commit&quot; [main 7681f85] first commit 6 files changed, 186 insertions(+) create mode 100644 lib/bowtie.nf create mode 100644 lib/fastqc.nf create mode 100644 lib/multiqc.nf create mode 100644 nextflow.config create mode 100644 params.config create mode 100755 main.nf [lcozzuto@nextflow test_course]$ git push Username for &#39;https://github.com&#39;: ###### Password for &#39;https://######@github.com&#39;: Counting objects: 10, done. Delta compression using up to 8 threads. Compressing objects: 100% (7/7), done. Writing objects: 100% (9/9), 2.62 KiB | 0 bytes/s, done. Total 9 (delta 0), reused 0 (delta 0) To https://github.com/lucacozzuto/test_course.git bbd6a44..7681f85 main -&gt; main So if we go again on the GitHub website we can see that eveything has been uploaded. Now we can remove that folder and go in the home folder. rm -fr test_course cd $HOME And we can launch directly this pipeline by typing nextflow run lucacozzuto/test_course -with-docker -r main \\ --reads &quot;/home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/testdata/*.fastq.gz&quot; \\ --reference &quot;/home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/testdata/chr19.fasta.gz&quot; As you can see we just use the repository name and two nextflow parameters: - -with-docker, for using Docker - -r, for using a specific branch. In this case the main branch. Then we pass to the pipelines the path of our input files: - –reads - –reference N E X T F L O W ~ version 20.10.0 Pulling lucacozzuto/test_course ... downloaded from https://github.com/lucacozzuto/test_course.git Launching `lucacozzuto/test_course` [voluminous_feynman] - revision: 95d1028adf [main] BIOCORE@CRG - N F TESTPIPE ~ version 1.0 ============================================= reads : /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/testdata/*.fastq.gz reference : /home/ec2-user/git/CoursesCRG_Containers_Nextflow_May_2021/nextflow/nextflow/testdata/chr19.fasta.gz executor &gt; local (5) [5b/4a36e8] process &gt; fastqc (B7_input_s_chr19.fastq.gz) [100%] 2 of 2 ✔ [5c/644577] process &gt; BOWTIE:bowtieIdx (chr19.fasta.gz) [100%] 1 of 1 ✔ executor &gt; local (5) [5b/4a36e8] process &gt; fastqc (B7_input_s_chr19.fastq.gz) [100%] 2 of 2 ✔ [5c/644577] process &gt; BOWTIE:bowtieIdx (chr19.fasta.gz) [100%] 1 of 1 ✔ [4b/dad392] process &gt; BOWTIE:bowtieAln (B7_input_s_chr19.fastq.gz) [100%] 2 of 2 ✔ /home/ec2-user/work/d1/11fe0bff99f424571033347bf4b042/B7_H3K4me1_s_chr19.fastq.gz.sam /home/ec2-user/work/4b/dad392b12d2f78f976d2a890ebcaea/B7_input_s_chr19.fastq.gz.sam Completed at: 27-Apr-2021 20:27:14 Duration : 1m 26s CPU hours : (a few seconds) Succeeded : 5 As you can see as first step, Nextflow pulls down the required version of the pipeline and it stores it at: /home/ec2-user/.nextflow/assets/lucacozzuto/test_course/ then it pulls the docker image and run the pipeline. You can use the Nextflow’s command list that show you the number of pipelines installed in your environment and the command info for fetching some useful information. nextflow list lucacozzuto/test_course ... nextflow info lucacozzuto/test_course project name: lucacozzuto/test_course repository : https://github.com/lucacozzuto/test_course local path : /home/ec2-user/.nextflow/assets/lucacozzuto/test_course main script : main.nf revision : * main Finally you can update, view or delete a project by using the Nextflow commands pull, view and drop. nextflow view lucacozzuto/test_course == content of file: /users/bi/lcozzuto/.nextflow/assets/lucacozzuto/test_course/main.nf #!/usr/bin/env nextflow /* * Copyright (c) 2013-2020, Centre for Genomic Regulation (CRG). * * This file is part of &#39;CRG_Containers_NextFlow&#39;. * * CRG_Containers_NextFlow is free software: you can redistribute it and/or modify * it under the terms of the GNU General Public License as published by * the Free Software Foundation, either version 3 of the License, or * (at your option) any later version. * * CRG_Containers_NextFlow is distributed in the hope that it will be useful, [...] "]]
